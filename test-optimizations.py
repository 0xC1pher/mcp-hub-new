#!/usr/bin/env python3
"""
Script de prueba para validar todas las optimizaciones implementadas
Ejecuta pruebas exhaustivas de cada componente optimizado
"""

import json
import time
import requests
import threading
import subprocess
import sys
from pathlib import Path

def log(message, status="INFO"):
    """Logging con colores"""
    colors = {
        "INFO": "\033[34m",  # Azul
        "SUCCESS": "\033[32m",  # Verde
        "WARNING": "\033[33m",  # Amarillo
        "ERROR": "\033[31m",   # Rojo
        "RESET": "\033[0m"
    }
    print(f"{colors.get(status, colors['RESET'])}[{status}] {message}{colors['RESET']}")

def test_server_startup():
    """Prueba el inicio del servidor optimizado"""
    log("ğŸš€ Probando inicio del servidor optimizado...")

    try:
        # Iniciar servidor en background
        server_process = subprocess.Popen([
            sys.executable,
            "servers/context-query/server.py",
            "8082"  # Puerto diferente para pruebas
        ], cwd=Path(__file__).parent, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        # Esperar que inicie
        time.sleep(3)

        # Verificar que estÃ¡ corriendo
        if server_process.poll() is None:
            log("âœ… Servidor iniciado correctamente", "SUCCESS")

            # Probar health endpoint
            try:
                response = requests.get("http://localhost:8082/health", timeout=5)
                if response.status_code == 200:
                    health_data = response.json()
                    log("âœ… Health check exitoso", "SUCCESS")
                    log(f"   VersiÃ³n: {health_data.get('version', 'N/A')}")
                    log(f"   Optimizaciones activas: {len(health_data.get('optimizations', {}))} mÃ³dulos")
                else:
                    log(f"âŒ Health check fallÃ³: {response.status_code}", "ERROR")
            except Exception as e:
                log(f"âŒ Error en health check: {e}", "ERROR")

            # Detener servidor
            server_process.terminate()
            server_process.wait(timeout=5)
            log("âœ… Servidor detenido correctamente", "SUCCESS")

            return True
        else:
            stdout, stderr = server_process.communicate()
            log(f"âŒ Servidor fallÃ³ al iniciar: {stderr.decode()}", "ERROR")
            return False

    except Exception as e:
        log(f"âŒ Error al probar servidor: {e}", "ERROR")
        return False

def test_optimizations():
    """Prueba todas las optimizaciones implementadas"""
    log("ğŸ§ª Probando optimizaciones implementadas...")

    try:
        # Importar mÃ³dulos de optimizaciÃ³n
        import sys
        import os
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'servers', 'context-query'))

        from optimizations import (
            token_budget, semantic_chunker, cache, query_optimizer,
            rate_limiter, resource_monitor, fuzzy_search, relevance_scorer
        )

        from spec_driven import SpecParser, SpecIndexer
        from document_loader import TrainingManager
        from reflector import Reflector
        from curator import Curator

        tests_passed = 0
        total_tests = 0

        # 1. Token Budgeting
        total_tests += 1
        try:
            sections = [
                {"content": "Esto es un test", "relevance": 0.8, "tokens": 10},
                {"content": "Otro contenido mÃ¡s largo para testing", "relevance": 0.6, "tokens": 15}
            ]
            allocated = token_budget.allocate_tokens(sections)
            if len(allocated) > 0:
                log("âœ… Token Budgeting funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Token Budgeting fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Token Budgeting: {e}", "ERROR")

        # 2. Semantic Chunking
        total_tests += 1
        try:
            text = "Este es un pÃ¡rrafo de prueba. Contiene varias oraciones. Cada una con diferente contenido semÃ¡ntico. Para probar el chunking avanzado."
            chunks = semantic_chunker.semantic_chunk(text)
            if len(chunks) > 0 and 'content' in chunks[0]:
                log("âœ… Semantic Chunking funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Semantic Chunking fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Semantic Chunking: {e}", "ERROR")

        # 3. Multi-level Cache
        total_tests += 1
        try:
            test_key = "test_cache_key"
            test_value = {"data": "test_value"}
            cache.set(test_key, test_value, ttl=60)
            retrieved = cache.get(test_key)
            if retrieved and retrieved['data'] == test_value['data']:
                log("âœ… Multi-level Cache funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Multi-level Cache fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Multi-level Cache: {e}", "ERROR")

        # 4. Query Optimization
        total_tests += 1
        try:
            result = query_optimizer.optimize_query("Â¿CÃ³mo funciona el cÃ³digo?")
            expected_keys = {"original_query", "normalized_query", "expanded_terms", "filtered_terms"}
            if expected_keys.issubset(result.keys()) and result['normalized_query']:
                log("âœ… Query Optimization funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Query Optimization fallÃ³: estructura inesperada", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Query Optimization: {e}", "ERROR")

        # 5. Rate Limiting
        total_tests += 1
        try:
            # Test bÃ¡sico de rate limiting
            allowed1 = rate_limiter.check_limit("127.0.0.1")
            allowed2 = rate_limiter.check_limit("127.0.0.1")
            if isinstance(allowed1, bool) and isinstance(allowed2, bool):
                log("âœ… Rate Limiting funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Rate Limiting fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Rate Limiting: {e}", "ERROR")

        # 6. Fuzzy Search
        total_tests += 1
        try:
            # Crear Ã­ndice de prueba
            test_docs = {
                "doc1": {"content": "python django model", "section_id": "coding", "section_title": "Coding"},
                "doc2": {"content": "seguridad autenticacion", "section_id": "security", "section_title": "Security"}
            }
            fuzzy_search.build_index(test_docs)

            if not fuzzy_search.has_index():
                raise AssertionError("Ãndice fuzzy no fue construido")

            results = fuzzy_search.search("python modelo")
            top_doc = results[0][0] if results else None

            if results and top_doc == "doc1":
                # Confirmar que podemos recuperar el documento original
                retrieved_doc = fuzzy_search.get_document(top_doc)
                if retrieved_doc and retrieved_doc.get("section_title") == "Coding":
                    log("âœ… Fuzzy Search funciona correctamente", "SUCCESS")
                    tests_passed += 1
                else:
                    log("âŒ Fuzzy Search fallÃ³ al recuperar documento", "ERROR")
            else:
                log("âŒ Fuzzy Search fallÃ³ en ranking", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Fuzzy Search: {e}", "ERROR")

        # 8. Spec-Driven Development (SpecParser)
        total_tests += 1
        try:
            parser = SpecParser()
            test_content = """
## User Stories
As a user, I want to login so that I can access my account.

## API Specifications
POST /api/login
Content-Type: application/json
{
  "username": "string",
  "password": "string"
}
"""
            specs = parser.parse_document(test_content, "test_doc.md")
            if 'user_stories' in specs and 'api_specifications' in specs:
                if len(specs['user_stories']) > 0 and len(specs['api_specifications']) > 0:
                    log("âœ… Spec-Driven Parser funciona correctamente", "SUCCESS")
                    tests_passed += 1
                else:
                    log("âŒ Spec-Driven Parser no extrajo specs", "ERROR")
            else:
                log("âŒ Spec-Driven Parser fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Spec-Driven Parser: {e}", "ERROR")

        # 9. Spec-Driven Development (SpecIndexer)
        total_tests += 1
        try:
            indexer = SpecIndexer()
            # Simular specs para indexar
            test_specs = {
                "test_doc.md": {
                    "user_stories": [{"content": "As a user I want to login", "confidence": 0.9}],
                    "api_specifications": [{"content": "POST /api/login", "confidence": 0.8}]
                }
            }
            indexer.index_specs(test_specs)
            results = indexer.search_specs("login")
            if results and len(results) > 0:
                log("âœ… Spec-Driven Indexer funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Spec-Driven Indexer fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Spec-Driven Indexer: {e}", "ERROR")

        # 10. Training Manager
        total_tests += 1
        try:
            import tempfile
            import os

            # Crear directorio temporal con archivo de prueba
            with tempfile.TemporaryDirectory() as temp_dir:
                test_file = os.path.join(temp_dir, "test.md")
                with open(test_file, "w") as f:
                    f.write("# Test Document\n\n## User Stories\nAs a user, I want to test.")

                manager = TrainingManager(temp_dir)
                result = manager.train_system()
                if result['status'] == 'trained':
                    log("âœ… Training Manager funciona correctamente", "SUCCESS")
                    tests_passed += 1
                else:
                    log("âŒ Training Manager fallÃ³", "ERROR")
        except Exception as e:
            log(f"âŒ Error en Training Manager: {e}", "ERROR")

        # 11. Reflector (ACE)
        total_tests += 1
        try:
            import tempfile

            # Crear archivo feedback temporal
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump([
                    {"query": "test", "response": "test response", "helpful": True},
                    {"query": "test2", "response": "test response 2", "helpful": False}
                ], f)
                feedback_file = f.name

            reflector_instance = Reflector(feedback_file)
            analysis = reflector_instance.analyze_feedback()
            if 'insights' in analysis:
                log("âœ… Reflector (ACE) funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Reflector (ACE) fallÃ³", "ERROR")

            os.unlink(feedback_file)
        except Exception as e:
            log(f"âŒ Error en Reflector (ACE): {e}", "ERROR")

        # 12. Curator (ACE)
        total_tests += 1
        try:
            import tempfile

            # Crear archivos temporales
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as idx_f:
                json.dump({"test": "index"}, idx_f)
                index_file = idx_f.name

            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as guide_f:
                json.dump({"test": "guidelines"}, guide_f)
                guidelines_file = guide_f.name

            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as fb_f:
                json.dump([], fb_f)
                feedback_file = fb_f.name

            curator_instance = Curator(index_file, guidelines_file, feedback_file)
            insights = [{"type": "missing_keyword", "keyword": "test"}]
            updates = curator_instance.apply_insights(insights)
            if isinstance(updates, list):
                log("âœ… Curator (ACE) funciona correctamente", "SUCCESS")
                tests_passed += 1
            else:
                log("âŒ Curator (ACE) fallÃ³", "ERROR")

            for f in [index_file, guidelines_file, feedback_file]:
                os.unlink(f)
        except Exception as e:
            log(f"âŒ Error en Curator (ACE): {e}", "ERROR")
        success_rate = (tests_passed / total_tests) * 100
        log(f"ğŸ“Š Tests completados: {tests_passed}/{total_tests} ({success_rate:.1f}%)", "SUCCESS" if success_rate >= 80 else "WARNING")

        return success_rate >= 80

    except Exception as e:
        log(f"âŒ Error general en pruebas de optimizaciÃ³n: {e}", "ERROR")
        return False

def main():
    """FunciÃ³n principal de pruebas"""
    log("ğŸ¯ Iniciando suite de pruebas del MCP Hub Optimizado")
    log("=" * 60)

    tests_passed = 0
    total_tests = 3  # servidor + optimizaciones bÃ¡sicas + nuevas tÃ©cnicas

    # 1. Prueba de servidor
    if test_server_startup():
        tests_passed += 1
        log("âœ… Test de servidor: PASÃ“", "SUCCESS")
    else:
        log("âŒ Test de servidor: FALLÃ“", "ERROR")

    log("-" * 40)

    # 2. Prueba de optimizaciones
    if test_optimizations():
        tests_passed += 1
        log("âœ… Tests de optimizaciÃ³n: PASÃ“", "SUCCESS")
    else:
        log("âŒ Tests de optimizaciÃ³n: FALLÃ“", "ERROR")

    # Resultado final
    log("=" * 60)
    final_success = (tests_passed == total_tests)
    if final_success:
        log("ğŸ‰ TODAS LAS PRUEBAS PASARON EXITOSAMENTE", "SUCCESS")
        log("ğŸš€ El MCP Hub Optimizado 2.0 estÃ¡ listo para producciÃ³n!")
    else:
        log(f"âš ï¸  {tests_passed}/{total_tests} pruebas pasaron", "WARNING")
        log("ğŸ”§ Revisar errores antes del despliegue")

    return final_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
