"""
Demo Integrado - Demostraci√≥n completa del sistema de caracter√≠sticas avanzadas
Este script muestra todas las caracter√≠sticas funcionando en conjunto de manera coordinada
"""

import asyncio
import time
import json
import os
import numpy as np
from typing import Dict, Any, List

# Importar el orquestador y configuraciones
try:
    from . import (
        create_orchestrator,
        create_comprehensive_config,
        ProcessingMode,
        AdvancedConfig
    )
    from .dynamic_chunking import ChunkType
    from .multi_vector_retrieval import VectorType
    from .query_expansion import QueryType
    from .confidence_calibration import CalibrationMethod
except ImportError:
    # Para ejecuci√≥n directa
    import sys
    sys.path.append(os.path.dirname(__file__))
    from __init__ import (
        create_orchestrator,
        create_comprehensive_config,
        ProcessingMode,
        AdvancedConfig
    )
    from dynamic_chunking import ChunkType
    from multi_vector_retrieval import VectorType
    from query_expansion import QueryType
    from confidence_calibration import CalibrationMethod


class IntegratedDemo:
    """Clase principal para la demostraci√≥n integrada"""

    def __init__(self):
        self.orchestrator = None
        self.demo_data = self._create_demo_data()

    def _create_demo_data(self) -> Dict[str, Any]:
        """Crea datos de ejemplo para la demostraci√≥n"""

        return {
            'documents': [
                {
                    'id': 'doc_ml_intro',
                    'content': """# Introducci√≥n a Machine Learning

Machine Learning es una rama de la inteligencia artificial que permite a las computadoras aprender y tomar decisiones a partir de datos sin ser programadas expl√≠citamente para cada tarea espec√≠fica.

## Tipos principales de Machine Learning

### 1. Aprendizaje Supervisado
El aprendizaje supervisado utiliza datos etiquetados para entrenar modelos. Los algoritmos aprenden de ejemplos de entrada-salida para hacer predicciones sobre nuevos datos no vistos.

Ejemplos comunes:
- Clasificaci√≥n de im√°genes
- Detecci√≥n de spam
- Predicci√≥n de precios
- Diagn√≥stico m√©dico

### 2. Aprendizaje No Supervisado
Este enfoque trabaja con datos sin etiquetas, buscando patrones ocultos o estructuras en los datos.

T√©cnicas principales:
- Clustering (K-means, DBSCAN)
- Reducci√≥n de dimensionalidad (PCA, t-SNE)
- Detecci√≥n de anomal√≠as

### 3. Aprendizaje por Refuerzo
El agente aprende a trav√©s de la interacci√≥n con un entorno, recibiendo recompensas o penalizaciones por sus acciones.

Aplicaciones:
- Juegos (AlphaGo, Chess)
- Veh√≠culos aut√≥nomos
- Sistemas de recomendaci√≥n
- Trading algor√≠tmico

## Algoritmos Fundamentales

```python
# Ejemplo de regresi√≥n lineal simple
import numpy as np
from sklearn.linear_model import LinearRegression

# Datos de ejemplo
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# Crear y entrenar el modelo
model = LinearRegression()
model.fit(X, y)

# Hacer predicciones
prediction = model.predict([[6]])
print(f"Predicci√≥n para x=6: {prediction[0]}")
```

## M√©tricas de Evaluaci√≥n

Para evaluar la calidad de nuestros modelos, utilizamos diferentes m√©tricas seg√∫n el tipo de problema:

### Clasificaci√≥n
- Accuracy (Exactitud)
- Precision (Precisi√≥n)
- Recall (Exhaustividad)
- F1-Score

### Regresi√≥n
- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)
- R-squared (Coeficiente de determinaci√≥n)

## Preprocesamiento de Datos

El preprocesamiento es crucial para el √©xito de cualquier proyecto de ML:

1. **Limpieza de datos**: Eliminar valores faltantes o err√≥neos
2. **Normalizaci√≥n**: Escalar las caracter√≠sticas a rangos similares
3. **Codificaci√≥n**: Convertir variables categ√≥ricas a num√©ricas
4. **Feature Engineering**: Crear nuevas caracter√≠sticas relevantes

## Desaf√≠os Comunes

- **Overfitting**: El modelo se ajusta demasiado a los datos de entrenamiento
- **Underfitting**: El modelo es demasiado simple para capturar patrones
- **Sesgo en los datos**: Los datos no son representativos de la poblaci√≥n
- **Interpretabilidad**: Entender c√≥mo el modelo toma decisiones

## Futuro del Machine Learning

El campo evoluciona r√°pidamente con avances en:
- Deep Learning y redes neuronales profundas
- Procesamiento de lenguaje natural (NLP)
- Computer Vision
- ML automatizado (AutoML)
- Inteligencia artificial explicable (XAI)
""",
                    'path': 'docs/machine_learning_intro.md',
                    'type': 'markdown',
                    'domain': 'machine_learning',
                    'complexity': 0.7
                },
                {
                    'id': 'doc_python_guide',
                    'content': """# Gu√≠a Pr√°ctica de Python para Data Science

Python se ha convertido en el lenguaje preferido para ciencia de datos y machine learning debido a su sintaxis clara y poderosas bibliotecas.

## Bibliotecas Esenciales

### NumPy - Computaci√≥n Num√©rica
```python
import numpy as np

# Crear arrays
arr = np.array([1, 2, 3, 4, 5])
matrix = np.array([[1, 2], [3, 4]])

# Operaciones matem√°ticas
result = np.sqrt(arr)
mean_val = np.mean(arr)

# Broadcasting
arr_2d = arr.reshape(5, 1)
broadcasted = arr_2d + arr
```

### Pandas - Manipulaci√≥n de Datos
```python
import pandas as pd

# Crear DataFrames
df = pd.DataFrame({
    'nombre': ['Ana', 'Luis', 'Mar√≠a'],
    'edad': [25, 30, 28],
    'salario': [50000, 60000, 55000]
})

# Operaciones b√°sicas
print(df.head())
print(df.describe())
print(df.groupby('edad').mean())

# Filtrado de datos
jovenes = df[df['edad'] < 30]
```

### Matplotlib y Seaborn - Visualizaci√≥n
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Gr√°fico b√°sico
plt.figure(figsize=(10, 6))
plt.plot([1, 2, 3, 4], [1, 4, 2, 3])
plt.title('Gr√°fico de L√≠nea')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

# Seaborn para gr√°ficos estad√≠sticos
sns.scatterplot(data=df, x='edad', y='salario')
plt.show()
```

### Scikit-learn - Machine Learning
```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Preparar datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entrenar modelo
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluar
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy:.4f}')
```

## Mejores Pr√°cticas

### 1. Organizaci√≥n del C√≥digo
```python
# Estructura recomendada para proyectos de DS
project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ external/
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ visualization/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ requirements.txt
```

### 2. Control de Versiones
- Usar Git para versionar c√≥digo
- Almacenar datos grandes separadamente
- Documentar cambios en modelos

### 3. Reproducibilidad
```python
# Fijar semillas aleatorias
import random
import numpy as np

def set_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    # Tambi√©n para frameworks como TensorFlow, PyTorch, etc.

set_seeds(42)
```

## Optimizaci√≥n de Rendimiento

### Vectorizaci√≥n con NumPy
```python
# Evitar bucles Python cuando sea posible
# Lento
result = []
for i in range(len(arr)):
    result.append(arr[i] ** 2)

# R√°pido
result = arr ** 2
```

### Uso Eficiente de Pandas
```python
# Usar m√©todos vectorizados
df['columna_nueva'] = df['columna1'] + df['columna2']

# Usar apply solo cuando sea necesario
df['processed'] = df['text_column'].apply(lambda x: x.upper())
```

## Debugging y Profiling

### IPython y Jupyter
```python
# Comandos m√°gicos √∫tiles
%time code_to_time()  # Tiempo de ejecuci√≥n
%timeit repeated_code()  # Tiempo promedio
%pdb  # Activar debugger
%matplotlib inline  # Gr√°ficos inline
```

### Logging
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Iniciando entrenamiento del modelo")
logger.warning("Par√°metro no √≥ptimo detectado")
logger.error("Error en validaci√≥n de datos")
```

Este enfoque sistem√°tico asegura c√≥digo mantenible y resultados reproducibles en proyectos de ciencia de datos.
""",
                    'path': 'guides/python_data_science.md',
                    'type': 'code',
                    'domain': 'programming',
                    'complexity': 0.8
                },
                {
                    'id': 'doc_neural_networks',
                    'content': """# Redes Neuronales Profundas: Conceptos y Aplicaciones

Las redes neuronales artificiales son modelos computacionales inspirados en el funcionamiento del cerebro humano, dise√±ados para reconocer patrones complejos en los datos.

## Fundamentos B√°sicos

### Perceptr√≥n Simple
El perceptr√≥n es la unidad b√°sica de una red neuronal:

```python
import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations

    def fit(self, X, y):
        # Inicializar pesos
        self.weights = np.zeros(1 + X.shape[1])
        self.costs = []

        for i in range(self.n_iterations):
            output = self.net_input(X)
            errors = y - output
            self.weights[1:] += self.learning_rate * X.T.dot(errors)
            self.weights[0] += self.learning_rate * errors.sum()
            cost = (errors**2).sum() / 2.0
            self.costs.append(cost)

    def net_input(self, X):
        return np.dot(X, self.weights[1:]) + self.weights[0]

    def activation(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, -1)
```

### Redes Multicapa (MLP)

```python
import tensorflow as tf
from tensorflow.keras import layers, models

# Definir arquitectura
model = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(10, activation='softmax')
])

# Compilar modelo
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Entrenar
history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=100,
    validation_split=0.2,
    verbose=1
)
```

## Arquitecturas Especializadas

### Redes Convolucionales (CNN)
Ideales para procesamiento de im√°genes:

```python
# CNN para clasificaci√≥n de im√°genes
cnn_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])
```

### Redes Recurrentes (RNN/LSTM)
Para secuencias y series temporales:

```python
# LSTM para predicci√≥n de series temporales
lstm_model = models.Sequential([
    layers.LSTM(50, return_sequences=True, input_shape=(timesteps, features)),
    layers.LSTM(50, return_sequences=False),
    layers.Dense(25),
    layers.Dense(1)
])
```

## T√©cnicas de Optimizaci√≥n

### Funciones de Activaci√≥n
- **ReLU**: f(x) = max(0, x) - M√°s com√∫n, evita vanishing gradient
- **Sigmoid**: f(x) = 1/(1+e^(-x)) - Salidas entre 0 y 1
- **Tanh**: f(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Salidas entre -1 y 1
- **Leaky ReLU**: f(x) = max(0.01x, x) - Permite gradientes negativos peque√±os

### Regularizaci√≥n
```python
# L1 y L2 Regularization
from tensorflow.keras import regularizers

model = models.Sequential([
    layers.Dense(128,
                activation='relu',
                kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.5),
    layers.Dense(64,
                activation='relu',
                kernel_regularizer=regularizers.l1(0.001)),
    layers.Dense(10, activation='softmax')
])
```

### Optimizadores Avanzados
```python
# Adam con learning rate scheduling
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7
)

model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
```

## T√©cnicas Avanzadas

### Transfer Learning
```python
# Usar modelo pre-entrenado
base_model = tf.keras.applications.VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Congelar capas base
base_model.trainable = False

# A√±adir capas personalizadas
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])
```

### Attention Mechanisms
```python
class AttentionLayer(layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(
            shape=(input_shape[-1], 1),
            initializer='random_normal',
            trainable=True
        )
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        e = tf.nn.tanh(tf.matmul(x, self.W))
        a = tf.nn.softmax(e, axis=1)
        output = x * a
        return tf.reduce_sum(output, axis=1)
```

## Monitoreo y Debugging

### Callbacks √ötiles
```python
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, TensorBoard
)

callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', save_best_only=True),
    TensorBoard(log_dir='./logs')
]

model.fit(X_train, y_train, callbacks=callbacks)
```

### Visualizaci√≥n de Arquitectura
```python
# Visualizar modelo
tf.keras.utils.plot_model(
    model,
    to_file='model.png',
    show_shapes=True,
    show_layer_names=True
)

# Resumen del modelo
model.summary()
```

## Aplicaciones Actuales

1. **Visi√≥n por Computadora**
   - Detecci√≥n de objetos (YOLO, R-CNN)
   - Segmentaci√≥n sem√°ntica (U-Net)
   - Generaci√≥n de im√°genes (GANs)

2. **Procesamiento de Lenguaje Natural**
   - Transformers (BERT, GPT)
   - Traducci√≥n autom√°tica
   - An√°lisis de sentimientos

3. **Reconocimiento de Voz**
   - Speech-to-text
   - S√≠ntesis de voz
   - Reconocimiento de hablante

4. **Sistemas de Recomendaci√≥n**
   - Filtrado colaborativo
   - Content-based filtering
   - Hybrid approaches

Las redes neuronales profundas han revolucionado el campo de la inteligencia artificial, permitiendo avances significativos en tareas que antes eran extremadamente dif√≠ciles para las computadoras.
""",
                    'path': 'advanced/neural_networks.md',
                    'type': 'technical',
                    'domain': 'deep_learning',
                    'complexity': 0.9
                }
            ],
            'queries': [
                "¬øC√≥mo funciona el machine learning?",
                "Mejores pr√°cticas para programar en Python",
                "Diferencia entre CNN y RNN",
                "¬øQu√© es regularizaci√≥n en redes neuronales?",
                "Tutorial de pandas para an√°lisis de datos",
                "Algoritmos de aprendizaje supervisado vs no supervisado",
                "C√≥mo implementar un perceptr√≥n desde cero"
            ]
        }

    async def run_comprehensive_demo(self):
        """Ejecuta la demostraci√≥n completa del sistema integrado"""

        print("üöÄ DEMO INTEGRADO - MCP HUB ENHANCED")
        print("=" * 80)
        print("Demostraci√≥n de todas las caracter√≠sticas avanzadas trabajando en conjunto")
        print()

        # 1. Inicializaci√≥n del sistema
        print("üìã FASE 1: Inicializaci√≥n del Sistema")
        print("-" * 40)

        print("Configurando sistema con modo COMPREHENSIVE...")
        config = create_comprehensive_config()
        self.orchestrator = create_orchestrator("comprehensive")

        # Mostrar configuraci√≥n
        print(f"   ‚úÖ Modo de procesamiento: {config.processing_mode.value}")
        print(f"   ‚úÖ Caracter√≠sticas habilitadas: {sum(1 for v in [
            config.enable_dynamic_chunking,
            config.enable_mvr,
            config.enable_virtual_chunks,
            config.enable_query_expansion,
            config.enable_confidence_calibration
        ] if v)}/5")
        print(f"   ‚úÖ Operaciones concurrentes: {config.max_concurrent_operations}")
        print(f"   ‚úÖ Resultados m√°ximos: {config.max_search_results}")

        # Estado inicial del sistema
        initial_status = self.orchestrator.get_system_status()
        print("\nüìä Estado inicial de caracter√≠sticas:")
        for feature, status in initial_status['feature_status'].items():
            emoji = "‚úÖ" if status == "enabled" else "‚ùå" if status == "error" else "‚è≥"
            print(f"   {emoji} {feature.replace('_', ' ').title()}: {status}")

        # 2. Preparaci√≥n de datos
        print(f"\nüìö FASE 2: Preparaci√≥n de Datos")
        print("-" * 40)

        print("Cargando documentos de ejemplo...")
        documents = self.demo_data['documents']

        for i, doc in enumerate(documents, 1):
            print(f"   {i}. {doc['id']}")
            print(f"      Tipo: {doc['type']} | Dominio: {doc['domain']}")
            print(f"      Tama√±o: {len(doc['content'])} chars | Complejidad: {doc['complexity']}")

        # 3. A√±adir documentos al sistema MVR (si est√° habilitado)
        if self.orchestrator.mvr_system:
            print(f"\nüîß Indexando documentos en sistema MVR...")
            for doc in documents:
                success = self.orchestrator.mvr_system.add_document(
                    doc_id=doc['id'],
                    content=doc['content'],
                    metadata={
                        'type': doc['type'],
                        'domain': doc['domain'],
                        'path': doc['path'],
                        'complexity': doc['complexity']
                    }
                )
                emoji = "‚úÖ" if success else "‚ùå"
                print(f"   {emoji} {doc['id']}")

        # 4. Procesamiento de queries
        print(f"\nüîç FASE 3: Procesamiento de Queries")
        print("-" * 40)

        queries = self.demo_data['queries'][:3]  # Primeras 3 queries para la demo

        for i, query in enumerate(queries, 1):
            print(f"\n>>> Query {i}: '{query}'")
            print("   " + "‚îÄ" * 50)

            start_time = time.time()

            # Procesamiento avanzado
            result = await self.orchestrator.process_advanced(
                query=query,
                documents=documents,
                context={'demo_query': i, 'timestamp': time.time()}
            )

            processing_time = time.time() - start_time

            # Mostrar resultados
            print(f"   ‚è±Ô∏è  Tiempo de procesamiento: {processing_time:.3f}s")
            print(f"   üîß Caracter√≠sticas usadas: {len([s for s in result.feature_status.values() if s.value == 'enabled'])}")

            # Query Expansion
            if result.expanded_queries:
                print(f"   üîÑ Queries expandidas ({len(result.expanded_queries)}):")
                for j, exp_query in enumerate(result.expanded_queries[:3], 1):
                    print(f"      {j}. {exp_query}")

            # Dynamic Chunking
            if result.chunks:
                print(f"   üìÑ Chunks generados: {len(result.chunks)}")
                chunk_types = {}
                for chunk in result.chunks:
                    if hasattr(chunk.metadata, 'chunk_type'):
                        chunk_type = chunk.metadata.chunk_type.value
                        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1

                for chunk_type, count in chunk_types.items():
                    print(f"      - {chunk_type}: {count} chunks")

            # Search Results
            if result.search_results:
                print(f"   üéØ Resultados de b√∫squeda ({len(result.search_results)}):")
                for j, search_result in enumerate(result.search_results[:3], 1):
                    print(f"      {j}. {search_result.doc_id} (Score: {search_result.score:.4f})")
                    if hasattr(search_result, 'vector_scores'):
                        vector_info = ', '.join([f"{k.value}: {v:.3f}" for k, v in list(search_result.vector_scores.items())[:2]])
                        print(f"         Vectores: {vector_info}")

            # Confidence Calibration
            if result.confidence_scores:
                print(f"   üéØ Calibraci√≥n de confianza:")
                for j, conf_score in enumerate(result.confidence_scores[:3], 1):
                    print(f"      {j}. Raw: {conf_score.raw_score:.3f} ‚Üí Calibrated: {conf_score.calibrated_score:.3f}")
                    print(f"         Nivel: {conf_score.confidence_level.value} | Incertidumbre: {conf_score.uncertainty_estimate:.3f}")

        # 5. Simulaci√≥n de feedback
        print(f"\nüîÑ FASE 4: Simulaci√≥n de Feedback")
        print("-" * 40)

        print("A√±adiendo feedback simulado para mejorar el sistema...")

        # Generar feedback sint√©tico
        np.random.seed(42)
        feedback_data = []

        for i, query in enumerate(queries):
            # Simular m√∫ltiples interacciones por query
            for j in range(5):
                relevance_score = np.random.beta(2, 1)  # Sesgado hacia scores altos
                was_helpful = relevance_score > 0.6  # Threshold para utilidad

                feedback_data.append({
                    'query': query,
                    'result_doc_id': f'doc_{i}_{j}',
                    'relevance_score': relevance_score,
                    'was_helpful': was_helpful
                })

                # A√±adir feedback al sistema
                self.orchestrator.add_feedback(
                    query=query,
                    result_doc_id=f'doc_{i}_{j}',
                    relevance_score=relevance_score,
                    was_helpful=was_helpful,
                    context={'simulation': True, 'query_idx': i}
                )

        print(f"   ‚úÖ A√±adido feedback para {len(feedback_data)} interacciones")
        print(f"   üìä Tasa de utilidad promedio: {np.mean([f['was_helpful'] for f in feedback_data]):.1%}")

        # 6. An√°lisis de rendimiento
        print(f"\nüìà FASE 5: An√°lisis de Rendimiento")
        print("-" * 40)

        final_status = self.orchestrator.get_system_status()

        print("üîß Estado de caracter√≠sticas:")
        enabled_features = final_status['config']['enabled_features']
        for feature in enabled_features:
            print(f"   ‚úÖ {feature.replace('_', ' ').title()}")

        print(f"\nüìä Estad√≠sticas de operaci√≥n:")
        stats = final_status['statistics']
        print(f"   ‚Ä¢ Total de operaciones: {stats['total_operations']}")
        print(f"   ‚Ä¢ Tiempo promedio: {stats['avg_processing_time_ms']:.1f}ms")

        if stats['feature_usage']:
            print(f"   ‚Ä¢ Uso por caracter√≠stica:")
            for feature, count in stats['feature_usage'].items():
                print(f"     - {feature.replace('_', ' ').title()}: {count} veces")

        if stats['error_counts']:
            print(f"   ‚Ä¢ Errores detectados:")
            for feature, errors in stats['error_counts'].items():
                print(f"     - {feature}: {errors} errores")

        # 7. M√©tricas de calibraci√≥n (si est√° disponible)
        if (self.orchestrator.confidence_calibrator and
            final_status.get('confidence_calibration_system')):

            print(f"\nüéØ M√©tricas de calibraci√≥n:")
            cc_status = final_status['confidence_calibration_system']

            if 'recent_metrics' in cc_status:
                metrics = cc_status['recent_metrics']
                print(f"   ‚Ä¢ Expected Calibration Error: {metrics.get('ece', 0):.4f}")
                print(f"   ‚Ä¢ Brier Score: {metrics.get('brier_score', 0):.4f}")
                print(f"   ‚Ä¢ Reliability Score: {metrics.get('reliability', 0):.4f}")

            print(f"   ‚Ä¢ Muestras de feedback: {cc_status.get('feedback_samples', 0)}")
            print(f"   ‚Ä¢ M√©todo actual: {cc_status.get('current_best_method', 'N/A')}")

        # 8. Optimizaci√≥n autom√°tica
        print(f"\n‚ö° FASE 6: Optimizaci√≥n Autom√°tica")
        print("-" * 40)

        optimization_report = self.orchestrator.optimize_configuration()

        print("üìä An√°lisis de rendimiento actual:")
        perf = optimization_report['current_performance']
        print(f"   ‚Ä¢ Tiempo promedio: {perf['avg_processing_time']:.3f}s")
        print(f"   ‚Ä¢ Operaciones totales: {perf['total_operations']}")

        if optimization_report['recommendations']:
            print(f"\nüí° Recomendaciones de optimizaci√≥n:")
            for i, rec in enumerate(optimization_report['recommendations'], 1):
                print(f"   {i}. {rec}")

        if optimization_report['auto_applied']:
            print(f"\nüîÑ Optimizaciones aplicadas autom√°ticamente:")
            for i, opt in enumerate(optimization_report['auto_applied'], 1):
                print(f"   {i}. {opt}")

        # 9. Demostraci√≥n de caracter√≠sticas espec√≠ficas
        print(f"\nüî¨ FASE 7: Demostraci√≥n de Caracter√≠sticas Espec√≠ficas")
        print("-" * 40)

        await self._demonstrate_specific_features()

        # 10. Resumen final
        print(f"\nüéâ RESUMEN FINAL")
        print("-" * 40)

        final_stats = self.orchestrator.get_system_status()

        print("‚úÖ Demostraci√≥n completada exitosamente!")
        print(f"\nüìã Caracter√≠sticas demostradas:")

        demos_completed = [
            "‚úÖ Dynamic Chunking Adaptativo",
            "‚úÖ Multi-Vector Retrieval (MVR)",
            "‚úÖ Query Expansion Autom√°tica",
            "‚úÖ Confidence Calibration Din√°mica",
            "‚úÖ Sistema Integrado de Orquestaci√≥n",
            "‚úÖ Feedback Loop y Optimizaci√≥n",
            "‚úÖ Procesamiento Paralelo",
            "‚úÖ M√©tricas y Monitoreo en Tiempo Real"
        ]

        for demo in demos_completed:
            print(f"   {demo}")

        print(f"\nüìä Estad√≠sticas finales del sistema:")
        print(f"   ‚Ä¢ Queries procesadas: {len(queries)}")
        print(f"   ‚Ä¢ Documentos indexados: {len(documents)}")
        print(f"   ‚Ä¢ Feedback recibido: {len(feedback_data)} interacciones")
        print(f"   ‚Ä¢ Caracter√≠sticas activas: {len(final_stats['config']['enabled_features'])}")
        print(f"   ‚Ä¢ Tiempo total de demo: {time.time() - start_time:.1f}s")

        print(f"\nüí° Pr√≥ximos pasos sugeridos:")
        print("   1. Integrar con fuentes de datos reales")
        print("   2. Ajustar configuraci√≥n seg√∫n casos de uso espec√≠ficos")
        print("   3. Implementar monitoreo continuo en producci√≥n")
        print("   4. Configurar pipelines de reentrenamiento autom√°tico")

    async def _demonstrate_specific_features(self):
        """Demuestra caracter√≠sticas espec√≠ficas en detalle"""

        print("üîß Demostraciones espec√≠ficas:")

        # 1. Dynamic Chunking con diferentes tipos de contenido
        if self.orchestrator.chunking_system:
            print("\n   üìÑ Dynamic Chunking:")

            test_content = """# T√≠tulo de prueba

Este es un p√°rrafo de ejemplo con contenido variado.

## Subsecci√≥n con c√≥digo

```python
def example_function():
    return "Hello, World!"
```

Y m√°s texto despu√©s del c√≥digo."""

            chunks = self.orchestrator.chunking_system.adaptive_chunking(
                text=test_content,
                file_path="test.md"
            )

            print(f"      ‚úÖ {len(chunks)} chunks generados")
            for i, chunk in enumerate(chunks, 1):
                print(f"         {i}. Tipo: {chunk.metadata.chunk_type.value}, Tama√±o: {chunk.metadata.size}")

        # 2. Query Expansion con diferentes tipos
        if self.orchestrator.query_expander:
            print("\n   üîÑ Query Expansion:")

            test_queries = [
                "¬øC√≥mo funciona el algoritmo?",
                "Mejores pr√°cticas de programaci√≥n",
                "Diferencias entre modelos"
            ]

            for query in test_queries:
                expansion = self.orchestrator.query_expander.expand_query(query, max_expansions=3)
                print(f"      '{query}' ‚Üí")
                print(f"         Tipo: {expansion.query_type.value}")
                print(f"         Expansiones: {len(expansion.expanded_terms)}")

        # 3. Confidence Calibration en acci√≥n
        if self.orchestrator.confidence_calibrator:
            print("\n   üéØ Confidence Calibration:")

            test_scores = [0.3, 0.6, 0.9]
            for score in test_scores:
                calibrated = self.orchestrator.confidence_calibrator.calibrate_confidence(score)
                print(f"      {score:.1f} ‚Üí {calibrated.calibrated_score:.3f} ({calibrated.confidence_level.value})")

        print("      ‚úÖ Demostraciones espec√≠ficas completadas")


def create_demo_config() -> AdvancedConfig:
    """Crea configuraci√≥n optimizada para la demo"""
    return AdvancedConfig(
        processing_mode=ProcessingMode.COMPREHENSIVE,
        max_concurrent_operations=4,
        cache_size_mb=50,
        enable_dynamic_chunking=True,
        enable_mvr=True,
        enable_virtual_chunks=False,  # Deshabilitado para simplicidad de demo
        enable_query_expansion=True,
        enable_confidence_calibration=True,
        max_search_results=8,
        max_expansions=6
    )


async def run_demo():
    """Funci√≥n principal para ejecutar la demo"""
    demo = IntegratedDemo()
    await demo.run_comprehensive_demo()


if __name__ == "__main__":
    """
    Ejecutar la demo integrada completa

    Este script demuestra todas las caracter√≠sticas avanzadas del MCP Hub Enhanced:

    1. Dynamic Chunking Adaptativo
    2. Multi-Vector Retrieval (MVR)
    3. Query Expansion Autom√°tica
    4. Confidence Calibration Din√°mica
    5. Sistema Integrado de Orquestaci√≥n

    Uso:
        python integrated_demo.py

    O desde el directorio padre:
        python -m core.advanced_features.integrated_demo
    """

    print("üöÄ Iniciando Demo Integrado del MCP Hub Enhanced...")
    print("   Preparando sistema avanzado con todas las caracter√≠sticas...")
    print()

    try:
        # Configurar logging
        import logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        # Ejecutar demo
        asyncio.run(run_demo())

        print("\nüéâ Demo completado exitosamente!")
        print("   Todas las caracter√≠sticas avanzadas han sido demostradas.")
        print("   El sistema est√° listo para integraci√≥n en producci√≥n.")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Demo interrumpido por el usuario")

    except Exception as e:
        print(f"\n‚ùå Error durante la demo: {e}")
        print("   Revisa los logs para m√°s detalles.")
        raise

    finally:
        print("\nüìã Demo finalizado")
        print("="*80)
